{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a LSTM for generating love poems.\n",
    "\n",
    "Love poems were collected from several online repositories. A LSTM was trained on a few hundred of these poems. Compromises on amount of training data and size of model were made due to memory limitations. Future directions for this project include streaming data to help mitigate this issue.\n",
    "\n",
    "The LSTM was trained in TensorFlow and leverage my local computer's GPU. Design of the script for training the model was heavily influenced by 2 training tutorials:\n",
    "\n",
    "\n",
    "Current model name and parameters:\n",
    "word_model_love_poems_composite_50.h5\n",
    "single layer, 600 nodes, batch size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import io\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend\n",
    "backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kyle\\\\Documents\\\\Blog Posts\\\\Autoencoder_for_text_generation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='love_poems_poem_hunter.txt'\n",
    "data1=(open(filename).read())\n",
    "data1=data1.lower()\n",
    "\n",
    "filename='love_poems_book_riot.txt'\n",
    "data2=(open(filename).read())\n",
    "data2=data2.lower()\n",
    "\n",
    "filename='love_poems_poetry_foundation.txt'\n",
    "data3=(open(filename).read())\n",
    "data3=data3.lower()\n",
    "\n",
    "filename='lovepoetry.txt'\n",
    "data4=open(filename).read()\n",
    "data4=data4.lower()\n",
    "#model_name='word_model_'+filename.split('.')[0]+'_composite.h5'\n",
    "model_name='word_model_love_poems_composite_50.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample from data4 because there are so many poems!\n",
    "random.seed(400)\n",
    "reshuffle=[elm for elm in data4.split('\\n\\n\\n\\n') if len(elm)>0]\n",
    "#reshuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(reshuffle)\n",
    "data4='\\n\\n\\n\\n'.join(reshuffle[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68405"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='\\n\\n\\n\\n'.join([data1,data2,data3,data4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1\n",
    "del data2\n",
    "del data3\n",
    "del data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.split('\\n\\n\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.lower().replace('\\n\\n\\n\\n','<endtoken>\\n<starttoken>')\n",
    "data=data.replace('\\n',' <returntoken> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302775"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, num_words=None):\n",
    "\n",
    "   # corpus = data.lower().split(\"\\n\")    \n",
    "    corpus = data.lower().replace('<endtoken>','<endtoken2><endtoken>').split(\"<endtoken>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    if num_words!=None:\n",
    "        tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= num_words} # <= because tokenizer is 1 indexed\n",
    "        tokenizer.word_index[tokenizer.oov_token] = num_words + 1\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, \n",
    "                 total_words,num_epochs=2,save=False, \n",
    "                 batch_size=100, device='/cpu:0'):\n",
    "    print(device)\n",
    "    input_len = max_sequence_len - 1\n",
    "    with tf.device(device):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "        #model.add(LSTM(250,return_sequences=True))\n",
    "        #model.add(Dropout(0.2))\n",
    "        #model.add(LSTM(500,return_sequences=True))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LSTM(600))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "        if save==True:\n",
    "            # define the checkpoint\n",
    "            filepath = model_name\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                                         save_best_only=True, mode='min')\n",
    "            #callbacks_list = [checkpoint]\n",
    "            callbacks_list = [EarlyStopping(monitor='loss', patience=2),checkpoint]\n",
    "            try:\n",
    "                model.load_weights(model_name)\n",
    "                print('found previous model. Loading weights.')\n",
    "            except:\n",
    "                print('No previous model found.')\n",
    "            model.fit(predictors, label, epochs=num_epochs, verbose=1,\n",
    "                      callbacks=callbacks_list,batch_size=batch_size)\n",
    "        else:\n",
    "            model.fit(predictors, label, epochs=num_epochs, verbose=1,\n",
    "                      batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    try:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "        #model.add(LSTM(350,return_sequences=True))\n",
    "        #model.add(Dropout(0.2))\n",
    "        #model.add(LSTM(500,return_sequences=True))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LSTM(600))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(total_words, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        # define the checkpoint\n",
    "        filepath =filename\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [EarlyStopping(monitor='loss', patience=2),checkpoint]\n",
    "        #callbacks_list = [checkpoint]\n",
    "        model.load_weights(filename)\n",
    "        print('found previous model. Loading weights.')\n",
    "    except:\n",
    "        print('No previous model found.')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, max_len, total_words = dataset_preparation(data, num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5129"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.483330083837005"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.split(' '))/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpu:0\n",
      "No previous model found.\n",
      "Epoch 1/1\n",
      "45218/45218 [==============================] - 1289s 29ms/step - loss: 6.4594\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.45938, saving model to word_model_love_poems_composite_50.h5\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X, Y, max_len, total_words,num_epochs=1,save=True, \n",
    "                     batch_size=50, device='/gpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss starting at 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_model(X, Y, max_len, total_words,num_epochs=1,save=False, device='/gpu:0')\n",
    "text = generate_text(\"starttoken\", 150, max_len, model)\n",
    "#print(text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(X, Y, max_len, total_words,num_epochs=60,save=True, \n",
    "                     batch_size=50, device='/gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpu:0\n",
      "found previous model. Loading weights.\n",
      "Epoch 1/10\n",
      "45218/45218 [==============================] - 1282s 28ms/step - loss: 0.5105\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.51051, saving model to word_model_love_poems_composite_50.h5\n",
      "Epoch 2/10\n",
      "45218/45218 [==============================] - 1273s 28ms/step - loss: 0.4673\n",
      "\n",
      "Epoch 00002: loss improved from 0.51051 to 0.46732, saving model to word_model_love_poems_composite_50.h5\n",
      "Epoch 3/10\n",
      "45218/45218 [==============================] - 1281s 28ms/step - loss: 0.4534\n",
      "\n",
      "Epoch 00003: loss improved from 0.46732 to 0.45343, saving model to word_model_love_poems_composite_50.h5\n",
      "Epoch 4/10\n",
      "45218/45218 [==============================] - 1272s 28ms/step - loss: 0.4713\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.45343\n",
      "Epoch 5/10\n",
      "45218/45218 [==============================] - 1268s 28ms/step - loss: 0.4843\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.45343\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X, Y, max_len, total_words,num_epochs=10,save=True, \n",
    "                     batch_size=50, device='/gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \n",
      "  \n",
      " i lie here in the night and dream of what will be \n",
      " when finally i am in your arm's with you lying next to me \n",
      " \n",
      " i dream of you do i are with my heart \n",
      " in you in the way you are to me \n",
      " \n",
      " why i love you because i love you \n",
      " and because i hate you i love you \n",
      " i love you because you are my life and my heart in my heart in my ocean \n",
      " \n",
      " i love you because you are my love and my poems \n",
      " love you because you are my sky and my moon in my lonely nights \n",
      " \n",
      " i love you because i love you \n",
      " and because you love you are my love and my poems \n",
      " love you because you are my sky and my\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(\"starttoken\", 150, max_len, model)\n",
    "print(text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       \n",
      "  \n",
      " i cannot be with another girl \n",
      " but john can't because he's her pearl \n",
      " to her it's all a great big game \n",
      " if you about a great part of this way \n",
      " \n",
      " i had a man love me \n",
      " but i thought i could do you \n",
      " \n",
      " i love you because i love you \n",
      " i am first you in me \n",
      " i can't believe you pushed me away \n",
      " i know you always be there \n",
      " but you know where i loved you \n",
      " i promise you to be forever \n",
      " \n",
      " you said if me that \n",
      " i can't know it is the way \n",
      " that makes me want to be \n",
      " and when i know that i love \n",
      " \n",
      " i love you because i love you \n",
      " and because i love you \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(\"starttoken\", 150, max_len, model)\n",
    "print(text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i love you \n",
      " i love love you \n",
      " no other words could say it more \n",
      " your handsome and clever \n",
      " without you i'd be sore \n",
      " \n",
      " i love you \n",
      " i love love you \n",
      " nothing else can say \n",
      " your charming and loving \n",
      " your soul delicate my inner \n",
      " \n",
      " i love our love \n",
      " when you was no part of my own \n",
      " \n",
      " i love you \n",
      " i love love you \n",
      " you are so loving of me \n",
      " and if you are a promise \n",
      " i shared i never never never fool \n",
      " i just never ask on the way \n",
      " \n",
      " now of i have away \n",
      " to the way of to my love \n",
      " and the beautiful times of a mountain \n",
      " \n",
      " i love you because i love you \n",
      " and because i love you i love you\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(\"starttoken\", 150, max_len, model)\n",
    "print(text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option to load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found previous model. Loading weights.\n"
     ]
    }
   ],
   "source": [
    "model=load_model(model_name, max_len, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "  love is love is love \n",
      " \n",
      " so t's the sky of the goodnight \n",
      " but you when it certain never \n",
      " \n",
      " love is to be now to you \n",
      " \n",
      " less who other do not on love \n",
      " love is no heart is on in white \n",
      " or love for love love love love \n",
      " \n",
      " love is not happy give love \n",
      " that is these love you will \n",
      " love is so much love love is the way \n",
      " \n",
      " love you want it much love love \n",
      " \n",
      " you are you the one \n",
      " you are the one and to give you \n",
      " not all your love and you \n",
      " when you just just the way you are \n"
     ]
    }
   ],
   "source": [
    "text = generate_text(\"starttoken\", 150, max_len, model)\n",
    "print(text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_writing(seed_text, model, top_n=10, length=10):\n",
    "    for x in range(0,length):\n",
    "        clear_output()\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])\n",
    "        token_list=token_list[0]\n",
    "        token_list =pad_sequences([token_list], maxlen= \n",
    "                                 max_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        predictions=pd.Series(model.predict(token_list, verbose=0)[0]\n",
    "            ).sort_values(ascending=False)[0:top_n]\n",
    "        word_dict=tokenizer.word_index\n",
    "        for i in range(0,len(predictions)):\n",
    "            word=list(word_dict.keys())[list(\n",
    "                word_dict.values()).index(predictions.index[i])]\n",
    "            weight=np.round(predictions.values[i],2)\n",
    "            print(i, weight, word)\n",
    "        print('Current sentence: %s'%(seed_text))\n",
    "        print('What should be the next word?')\n",
    "        \n",
    "        try:\n",
    "            choice=int(input())\n",
    "            output_word=list(word_dict.keys())[list(\n",
    "                word_dict.values()).index(predictions.index[choice])]\n",
    "            seed_text += \" \" + output_word\n",
    "        except:\n",
    "            print('early end')\n",
    "            break\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.91 i\n",
      "1 0.07 for\n",
      "2 0.01 returntoken\n",
      "3 0.0 love\n",
      "4 0.0 to\n",
      "Current sentence: starttoken i love not because i love you returntoken i go from loving to not loving you returntoken from waiting to not waiting for you returntoken my heart moves from cold to fire returntoken returntoken you are a reason returntoken that fills each season returntoken when i love you returntoken\n",
      "What should be the next word?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "seed_text='starttoken'\n",
    "seed_text=augmented_writing(seed_text, model, top_n=5, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i love not because i love you \n",
      " i go from loving to not loving you \n",
      " from waiting to not waiting for you \n",
      " my heart moves from cold to fire \n",
      " \n",
      " you are a reason \n",
      " that fills each season \n",
      " when i love you \n",
      " i\n"
     ]
    }
   ],
   "source": [
    "print(seed_text.replace('starttoken','').replace('returntoken','\\n').split('endtoken2')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
